{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba691d6-f957-4548-82aa-863a49dbfc68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0f847163-a24d-4ccc-9328-bff2ff697b68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324998b4-2219-4954-a7b4-d36ea425e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Scraping page 2...\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Scraping page 3...\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Scraping page 4...\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Scraping page 5...\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "Skipped due to error: 'NoneType' object has no attribute 'text'\n",
      "✅ Data saved to stackoverflow_scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "base_url = \"https://stackoverflow.com\"\n",
    "page_url = \"https://stackoverflow.com/questions?tab=newest&page=\"\n",
    "\n",
    "titles = []\n",
    "bodies = []\n",
    "tags_list = []\n",
    "\n",
    "for page in range(1, 6):  # You can increase to scrape more pages\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    response = requests.get(page_url + str(page))\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    questions = soup.select(\".s-post-summary--content\")\n",
    "\n",
    "    for q in questions:\n",
    "        try:\n",
    "            title = q.select_one(\".s-link\").text.strip()\n",
    "            question_url = base_url + q.select_one(\".s-link\")['href']\n",
    "            \n",
    "            q_res = requests.get(question_url)\n",
    "            q_soup = BeautifulSoup(q_res.text, \"html.parser\")\n",
    "            \n",
    "            body = q_soup.select_one(\".js-post-body\").text.strip()\n",
    "            tags = [tag.text for tag in q_soup.select(\".post-tag\")]\n",
    "\n",
    "            titles.append(title)\n",
    "            bodies.append(body)\n",
    "            tags_list.append(tags)\n",
    "\n",
    "            time.sleep(1)  # Sleep to avoid being blocked\n",
    "        except Exception as e:\n",
    "            print(\"Skipped due to error:\", e)\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Body\": bodies,\n",
    "    \"Tags\": tags_list\n",
    "})\n",
    "\n",
    "df.to_csv(\"stackoverflow_scraped_data.csv\", index=False)\n",
    "print(\"✅ Data saved to stackoverflow_scraped_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e30e3a8-12d8-4a89-ad55-6a20c7ff7787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'stackoverflow_scraped_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     49\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: titles,\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m: bodies,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags_list\n\u001b[0;32m     53\u001b[0m })\n\u001b[1;32m---> 55\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstackoverflow_scraped_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved to stackoverflow_scraped_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'stackoverflow_scraped_data.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "base_url = \"https://stackoverflow.com\"\n",
    "page_url = \"https://stackoverflow.com/questions?tab=newest&page=\"\n",
    "\n",
    "titles = []\n",
    "bodies = []\n",
    "tags_list = []\n",
    "\n",
    "for page in range(1, 4):  # Adjust range for more data\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    response = requests.get(page_url + str(page))\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    questions = soup.select(\".s-post-summary--content\")\n",
    "\n",
    "    for q in questions:\n",
    "        try:\n",
    "            title_tag = q.select_one(\".s-link\")\n",
    "            if not title_tag:\n",
    "                continue\n",
    "            title = title_tag.text.strip()\n",
    "            question_url = base_url + title_tag['href']\n",
    "            \n",
    "            q_res = requests.get(question_url)\n",
    "            q_soup = BeautifulSoup(q_res.text, \"html.parser\")\n",
    "            \n",
    "            body_tag = q_soup.select_one(\".js-post-body\")\n",
    "            if not body_tag:\n",
    "                continue\n",
    "            body = body_tag.text.strip()\n",
    "\n",
    "            tags = [tag.text for tag in q_soup.select(\".post-tag\")]\n",
    "            if not tags:\n",
    "                continue\n",
    "\n",
    "            titles.append(title)\n",
    "            bodies.append(body)\n",
    "            tags_list.append(tags)\n",
    "\n",
    "            time.sleep(1)  # Respectful delay\n",
    "        except Exception as e:\n",
    "            print(\"Skipped due to error:\", e)\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Body\": bodies,\n",
    "    \"Tags\": tags_list\n",
    "})\n",
    "\n",
    "df.to_csv(\"stackoverflow_scraped_data.csv\", index=False)\n",
    "print(\"✅ Saved to stackoverflow_scraped_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006396d2-4846-451e-a8e4-1b5f32a8dfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "✅ Done: Data saved to stackoverflow_scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "base_url = \"https://stackoverflow.com\"\n",
    "page_url = \"https://stackoverflow.com/questions?tab=newest&page=\"\n",
    "\n",
    "titles, bodies, tags_list = [], [], []\n",
    "\n",
    "for page in range(1, 6):  # Change range for more data\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    response = requests.get(page_url + str(page), headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    questions = soup.select(\".s-post-summary--content\")\n",
    "\n",
    "    for q in questions:\n",
    "        try:\n",
    "            title_tag = q.select_one(\".s-link\")\n",
    "            if not title_tag:\n",
    "                continue\n",
    "\n",
    "            title = title_tag.text.strip()\n",
    "            question_url = base_url + title_tag['href']\n",
    "\n",
    "            q_res = requests.get(question_url, headers=headers)\n",
    "            q_soup = BeautifulSoup(q_res.text, \"html.parser\")\n",
    "\n",
    "            body_tag = q_soup.select_one(\".js-post-body\")\n",
    "            if not body_tag:\n",
    "                continue\n",
    "\n",
    "            body = body_tag.text.strip()\n",
    "            tags = [tag.text for tag in q_soup.select(\".post-tag\")]\n",
    "\n",
    "            if not tags:\n",
    "                continue\n",
    "\n",
    "            titles.append(title)\n",
    "            bodies.append(body)\n",
    "            tags_list.append(tags)\n",
    "\n",
    "            time.sleep(1.5)  # Delay to avoid bans\n",
    "        except Exception as e:\n",
    "            print(\"Skipped due to error:\", e)\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Body\": bodies,\n",
    "    \"Tags\": tags_list\n",
    "})\n",
    "\n",
    "df.to_csv(\"scraped_data.csv\", index=False)\n",
    "print(\"✅ Done: Data saved to stackoverflow_scraped_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d654643f-9adc-4e28-b140-87ef72fc10f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Scraping page 1...\n",
      "🔢 Found 15 questions on page 1.\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "\n",
      "🔍 Scraping page 2...\n",
      "🔢 Found 15 questions on page 2.\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "❌ No body found, skipping...\n",
      "\n",
      "📊 Total collected: 0 questions.\n",
      "⚠️ No data collected. Check HTML selectors or if IP is blocked.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "base_url = \"https://stackoverflow.com\"\n",
    "page_url = \"https://stackoverflow.com/questions?tab=newest&page=\"\n",
    "\n",
    "titles, bodies, tags_list = [], [], []\n",
    "\n",
    "for page in range(1, 3):  # Start with 2 pages for test\n",
    "    print(f\"\\n🔍 Scraping page {page}...\")\n",
    "    response = requests.get(page_url + str(page), headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    questions = soup.select(\".s-post-summary--content\")\n",
    "    print(f\"🔢 Found {len(questions)} questions on page {page}.\")\n",
    "\n",
    "    for q in questions:\n",
    "        try:\n",
    "            title_tag = q.select_one(\".s-link\")\n",
    "            if not title_tag:\n",
    "                print(\"❌ No title found, skipping...\")\n",
    "                continue\n",
    "\n",
    "            title = title_tag.text.strip()\n",
    "            question_url = base_url + title_tag['href']\n",
    "\n",
    "            q_res = requests.get(question_url, headers=headers)\n",
    "            q_soup = BeautifulSoup(q_res.text, \"html.parser\")\n",
    "\n",
    "            body_tag = q_soup.select_one(\".js-post-body\")\n",
    "            if not body_tag:\n",
    "                print(\"❌ No body found, skipping...\")\n",
    "                continue\n",
    "\n",
    "            body = body_tag.text.strip()\n",
    "            tags = [tag.text for tag in q_soup.select(\".post-tag\")]\n",
    "            if not tags:\n",
    "                print(\"❌ No tags found, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"✅ Question: {title[:40]}... | Tags: {tags}\")\n",
    "            titles.append(title)\n",
    "            bodies.append(body)\n",
    "            tags_list.append(tags)\n",
    "\n",
    "            time.sleep(1.2)\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error:\", e)\n",
    "            continue\n",
    "\n",
    "print(f\"\\n📊 Total collected: {len(titles)} questions.\")\n",
    "if titles:\n",
    "    df = pd.DataFrame({\n",
    "        \"Title\": titles,\n",
    "        \"Body\": bodies,\n",
    "        \"Tags\": tags_list\n",
    "    })\n",
    "    df.to_csv(\"Scraped_data.csv\", index=False)\n",
    "    print(\"✅ Saved to 'stackoverflow_scraped_data.csv'\")\n",
    "else:\n",
    "    print(\"⚠️ No data collected. Check HTML selectors or if IP is blocked.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5023b94c-e9dc-455e-b156-db8cac23ff35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Fetching page 6...\n",
      "Fetching page 7...\n",
      "Fetching page 8...\n",
      "Fetching page 9...\n",
      "Fetching page 10...\n",
      "Fetching page 11...\n",
      "Fetching page 12...\n",
      "Fetching page 13...\n",
      "Fetching page 14...\n",
      "Fetching page 15...\n",
      "Fetching page 16...\n",
      "Fetching page 17...\n",
      "Fetching page 18...\n",
      "Fetching page 19...\n",
      "Fetching page 20...\n",
      "Fetching page 21...\n",
      "Fetching page 22...\n",
      "Fetching page 23...\n",
      "Fetching page 24...\n",
      "Fetching page 25...\n",
      "Fetching page 26...\n",
      "❌ No items – API error or rate limit.\n",
      "Total questions fetched: 2500\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'stackoverflow_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 41\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal questions fetched: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(titles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: titles,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m: bodies,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags_list\n\u001b[0;32m     40\u001b[0m })\n\u001b[1;32m---> 41\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstackoverflow_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstackoverflow_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'stackoverflow_data.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "titles, bodies, tags_list = [], [], []\n",
    "\n",
    "for page in range(1, 40):  # start small for testing (3 pages × 100 = 300 questions)\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    resp = requests.get(\n",
    "        \"https://api.stackexchange.com/2.3/questions\",\n",
    "        params={\n",
    "            \"page\": page,\n",
    "            \"pagesize\": 100,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"filter\": \"withbody\"\n",
    "        }\n",
    "    )\n",
    "    data = resp.json()\n",
    "    if \"items\" not in data:\n",
    "        print(\"❌ No items – API error or rate limit.\")\n",
    "        break\n",
    "\n",
    "    for item in data[\"items\"]:\n",
    "        titles.append(item.get(\"title\", \"\"))\n",
    "        # Strip HTML from body\n",
    "        body_html = item.get(\"body\", \"\")\n",
    "        bodies.append(BeautifulSoup(body_html, \"html.parser\").get_text())\n",
    "        tags_list.append(item.get(\"tags\", []))\n",
    "\n",
    "    time.sleep(1.2)  # respect rate limits\n",
    "\n",
    "print(f\"Total questions fetched: {len(titles)}\")\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Body\": bodies,\n",
    "    \"Tags\": tags_list\n",
    "})\n",
    "df.to_csv(\"stackoverflow_data.csv\", index=False)\n",
    "print(\"✅ Saved to 'stackoverflow_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52833491-9037-42d4-9ff2-5590c13535ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Fetching page 1 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 100\n",
      "📦 Fetching page 2 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 200\n",
      "📦 Fetching page 3 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 300\n",
      "📦 Fetching page 4 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 400\n",
      "📦 Fetching page 5 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 500\n",
      "📦 Fetching page 6 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 600\n",
      "📦 Fetching page 7 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 700\n",
      "📦 Fetching page 8 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 800\n",
      "📦 Fetching page 9 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 900\n",
      "📦 Fetching page 10 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1000\n",
      "📦 Fetching page 11 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1100\n",
      "📦 Fetching page 12 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1200\n",
      "📦 Fetching page 13 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1300\n",
      "📦 Fetching page 14 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1400\n",
      "📦 Fetching page 15 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1500\n",
      "📦 Fetching page 16 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1600\n",
      "📦 Fetching page 17 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1700\n",
      "📦 Fetching page 18 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1800\n",
      "📦 Fetching page 19 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 1900\n",
      "📦 Fetching page 20 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 2000\n",
      "📦 Fetching page 21 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 2100\n",
      "📦 Fetching page 22 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 2200\n",
      "📦 Fetching page 23 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 2300\n",
      "📦 Fetching page 24 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 2400\n",
      "📦 Fetching page 25 from 1577836800 to 1593561600...\n",
      "✅ Collected so far: 2500\n",
      "📦 Fetching page 26 from 1577836800 to 1593561600...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "📦 Fetching page 1 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 2600\n",
      "📦 Fetching page 2 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 2700\n",
      "📦 Fetching page 3 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 2800\n",
      "📦 Fetching page 4 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 2900\n",
      "📦 Fetching page 5 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3000\n",
      "📦 Fetching page 6 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3100\n",
      "📦 Fetching page 7 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3200\n",
      "📦 Fetching page 8 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3300\n",
      "📦 Fetching page 9 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3400\n",
      "📦 Fetching page 10 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3500\n",
      "📦 Fetching page 11 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3600\n",
      "📦 Fetching page 12 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3700\n",
      "📦 Fetching page 13 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3800\n",
      "📦 Fetching page 14 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 3900\n",
      "📦 Fetching page 15 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4000\n",
      "📦 Fetching page 16 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4100\n",
      "📦 Fetching page 17 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4200\n",
      "📦 Fetching page 18 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4300\n",
      "📦 Fetching page 19 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4400\n",
      "📦 Fetching page 20 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4500\n",
      "📦 Fetching page 21 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4600\n",
      "📦 Fetching page 22 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4700\n",
      "📦 Fetching page 23 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4800\n",
      "📦 Fetching page 24 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 4900\n",
      "📦 Fetching page 25 from 1593561601 to 1609459200...\n",
      "✅ Collected so far: 5000\n",
      "📦 Fetching page 26 from 1593561601 to 1609459200...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "📦 Fetching page 1 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5100\n",
      "📦 Fetching page 2 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5200\n",
      "📦 Fetching page 3 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5300\n",
      "📦 Fetching page 4 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5400\n",
      "📦 Fetching page 5 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5500\n",
      "📦 Fetching page 6 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5600\n",
      "📦 Fetching page 7 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5700\n",
      "📦 Fetching page 8 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5800\n",
      "📦 Fetching page 9 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 5900\n",
      "📦 Fetching page 10 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6000\n",
      "📦 Fetching page 11 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6100\n",
      "📦 Fetching page 12 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6200\n",
      "📦 Fetching page 13 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6300\n",
      "📦 Fetching page 14 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6400\n",
      "📦 Fetching page 15 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6500\n",
      "📦 Fetching page 16 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6600\n",
      "📦 Fetching page 17 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6700\n",
      "📦 Fetching page 18 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6800\n",
      "📦 Fetching page 19 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 6900\n",
      "📦 Fetching page 20 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 7000\n",
      "📦 Fetching page 21 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 7100\n",
      "📦 Fetching page 22 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 7200\n",
      "📦 Fetching page 23 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 7300\n",
      "📦 Fetching page 24 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 7400\n",
      "📦 Fetching page 25 from 1609459201 to 1625097600...\n",
      "✅ Collected so far: 7500\n",
      "📦 Fetching page 26 from 1609459201 to 1625097600...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "📦 Fetching page 1 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 7600\n",
      "📦 Fetching page 2 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 7700\n",
      "📦 Fetching page 3 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 7800\n",
      "📦 Fetching page 4 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 7900\n",
      "📦 Fetching page 5 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8000\n",
      "📦 Fetching page 6 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8100\n",
      "📦 Fetching page 7 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8200\n",
      "📦 Fetching page 8 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8300\n",
      "📦 Fetching page 9 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8400\n",
      "📦 Fetching page 10 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8500\n",
      "📦 Fetching page 11 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8600\n",
      "📦 Fetching page 12 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8700\n",
      "📦 Fetching page 13 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8800\n",
      "📦 Fetching page 14 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 8900\n",
      "📦 Fetching page 15 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9000\n",
      "📦 Fetching page 16 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9100\n",
      "📦 Fetching page 17 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9200\n",
      "📦 Fetching page 18 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9300\n",
      "📦 Fetching page 19 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9400\n",
      "📦 Fetching page 20 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9500\n",
      "📦 Fetching page 21 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9600\n",
      "📦 Fetching page 22 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9700\n",
      "📦 Fetching page 23 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9800\n",
      "📦 Fetching page 24 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 9900\n",
      "📦 Fetching page 25 from 1625097601 to 1640995200...\n",
      "✅ Collected so far: 10000\n",
      "📦 Fetching page 26 from 1625097601 to 1640995200...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "📦 Fetching page 1 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10100\n",
      "📦 Fetching page 2 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10200\n",
      "📦 Fetching page 3 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10300\n",
      "📦 Fetching page 4 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10400\n",
      "📦 Fetching page 5 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10500\n",
      "📦 Fetching page 6 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10600\n",
      "📦 Fetching page 7 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10700\n",
      "📦 Fetching page 8 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10800\n",
      "📦 Fetching page 9 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 10900\n",
      "📦 Fetching page 10 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11000\n",
      "📦 Fetching page 11 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11100\n",
      "📦 Fetching page 12 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11200\n",
      "📦 Fetching page 13 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11300\n",
      "📦 Fetching page 14 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11400\n",
      "📦 Fetching page 15 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11500\n",
      "📦 Fetching page 16 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11600\n",
      "📦 Fetching page 17 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11700\n",
      "📦 Fetching page 18 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11800\n",
      "📦 Fetching page 19 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 11900\n",
      "📦 Fetching page 20 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 12000\n",
      "📦 Fetching page 21 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 12100\n",
      "📦 Fetching page 22 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 12200\n",
      "📦 Fetching page 23 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 12300\n",
      "📦 Fetching page 24 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 12400\n",
      "📦 Fetching page 25 from 1672531200 to 1688169600...\n",
      "✅ Collected so far: 12500\n",
      "📦 Fetching page 26 from 1672531200 to 1688169600...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "📦 Fetching page 1 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 12600\n",
      "📦 Fetching page 2 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 12700\n",
      "📦 Fetching page 3 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 12800\n",
      "📦 Fetching page 4 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 12900\n",
      "📦 Fetching page 5 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13000\n",
      "📦 Fetching page 6 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13100\n",
      "📦 Fetching page 7 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13200\n",
      "📦 Fetching page 8 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13300\n",
      "📦 Fetching page 9 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13400\n",
      "📦 Fetching page 10 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13500\n",
      "📦 Fetching page 11 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13600\n",
      "📦 Fetching page 12 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13700\n",
      "📦 Fetching page 13 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13800\n",
      "📦 Fetching page 14 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 13900\n",
      "📦 Fetching page 15 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14000\n",
      "📦 Fetching page 16 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14100\n",
      "📦 Fetching page 17 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14200\n",
      "📦 Fetching page 18 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14300\n",
      "📦 Fetching page 19 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14400\n",
      "📦 Fetching page 20 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14500\n",
      "📦 Fetching page 21 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14600\n",
      "📦 Fetching page 22 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14700\n",
      "📦 Fetching page 23 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14800\n",
      "📦 Fetching page 24 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 14900\n",
      "📦 Fetching page 25 from 1688169601 to 1704067200...\n",
      "✅ Collected so far: 15000\n",
      "📦 Fetching page 26 from 1688169601 to 1704067200...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "📦 Fetching page 1 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15100\n",
      "📦 Fetching page 2 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15200\n",
      "📦 Fetching page 3 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15300\n",
      "📦 Fetching page 4 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15400\n",
      "📦 Fetching page 5 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15500\n",
      "📦 Fetching page 6 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15600\n",
      "📦 Fetching page 7 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15700\n",
      "📦 Fetching page 8 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15800\n",
      "📦 Fetching page 9 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 15900\n",
      "📦 Fetching page 10 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16000\n",
      "📦 Fetching page 11 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16100\n",
      "📦 Fetching page 12 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16200\n",
      "📦 Fetching page 13 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16300\n",
      "📦 Fetching page 14 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16400\n",
      "📦 Fetching page 15 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16500\n",
      "📦 Fetching page 16 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16600\n",
      "📦 Fetching page 17 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16700\n",
      "📦 Fetching page 18 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16800\n",
      "📦 Fetching page 19 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 16900\n",
      "📦 Fetching page 20 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 17000\n",
      "📦 Fetching page 21 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 17100\n",
      "📦 Fetching page 22 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 17200\n",
      "📦 Fetching page 23 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 17300\n",
      "📦 Fetching page 24 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 17400\n",
      "📦 Fetching page 25 from 1704067201 to 1719792000...\n",
      "✅ Collected so far: 17500\n",
      "📦 Fetching page 26 from 1704067201 to 1719792000...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "📦 Fetching page 1 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 17600\n",
      "📦 Fetching page 2 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 17700\n",
      "📦 Fetching page 3 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 17800\n",
      "📦 Fetching page 4 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 17900\n",
      "📦 Fetching page 5 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18000\n",
      "📦 Fetching page 6 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18100\n",
      "📦 Fetching page 7 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18200\n",
      "📦 Fetching page 8 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18300\n",
      "📦 Fetching page 9 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18400\n",
      "📦 Fetching page 10 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18500\n",
      "📦 Fetching page 11 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18600\n",
      "📦 Fetching page 12 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18700\n",
      "📦 Fetching page 13 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18800\n",
      "📦 Fetching page 14 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 18900\n",
      "📦 Fetching page 15 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19000\n",
      "📦 Fetching page 16 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19100\n",
      "📦 Fetching page 17 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19200\n",
      "📦 Fetching page 18 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19300\n",
      "📦 Fetching page 19 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19400\n",
      "📦 Fetching page 20 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19500\n",
      "📦 Fetching page 21 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19600\n",
      "📦 Fetching page 22 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19700\n",
      "📦 Fetching page 23 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19800\n",
      "📦 Fetching page 24 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 19900\n",
      "📦 Fetching page 25 from 1719792001 to 1735689600...\n",
      "✅ Collected so far: 20000\n",
      "📦 Fetching page 26 from 1719792001 to 1735689600...\n",
      "❌ API limit hit or error: {'error_id': 403, 'error_message': 'page above 25 requires access token or app key', 'error_name': 'access_denied'}\n",
      "✅ Done! Saved to 'stackoverflow_2020_2021_2023_2024.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html):\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "titles, bodies, tags_list = [], [], []\n",
    "\n",
    "# Yearly ranges: Jan–Jun and Jul–Dec for 2020, 2021, 2023, 2024\n",
    "date_ranges = [\n",
    "    (1577836800, 1593561600),  # Jan–Jun 2020\n",
    "    (1593561601, 1609459200),  # Jul–Dec 2020\n",
    "    (1609459201, 1625097600),  # Jan–Jun 2021\n",
    "    (1625097601, 1640995200),  # Jul–Dec 2021\n",
    "    (1672531200, 1688169600),  # Jan–Jun 2023\n",
    "    (1688169601, 1704067200),  # Jul–Dec 2023\n",
    "    (1704067201, 1719792000),  # Jan–Jun 2024\n",
    "    (1719792001, 1735689600),  # Jul–Dec 2024 (future-safe)\n",
    "]\n",
    "\n",
    "api_key = \"\"  # Optional: add your key here\n",
    "\n",
    "for start, end in date_ranges:\n",
    "    for page in range(1, 41):  # Up to 2000 questions per range\n",
    "        print(f\"📦 Fetching page {page} from {start} to {end}...\")\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": 100,\n",
    "            \"fromdate\": start,\n",
    "            \"todate\": end,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"filter\": \"withbody\"\n",
    "        }\n",
    "        if api_key:\n",
    "            params[\"key\"] = api_key\n",
    "\n",
    "        resp = requests.get(\"https://api.stackexchange.com/2.3/questions\", params=params)\n",
    "        data = resp.json()\n",
    "\n",
    "        if \"items\" not in data:\n",
    "            print(\"❌ API limit hit or error:\", data)\n",
    "            break\n",
    "\n",
    "        for item in data[\"items\"]:\n",
    "            titles.append(item.get(\"title\", \"\"))\n",
    "            bodies.append(clean_html(item.get(\"body\", \"\")))\n",
    "            tags_list.append(item.get(\"tags\", []))\n",
    "\n",
    "        print(f\"✅ Collected so far: {len(titles)}\")\n",
    "        time.sleep(1.2)  # Respect rate limit\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Body\": bodies,\n",
    "    \"Tags\": tags_list\n",
    "})\n",
    "df.to_csv(\"stackoverflow_2020_2021_2023_2024.csv\", index=False)\n",
    "print(\"✅ Done! Saved to 'stackoverflow_2020_2021_2023_2024.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da7d0e2c-db4b-4efe-a2a5-95de5b9033ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Fetching page 1 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 100\n",
      "📦 Fetching page 2 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 200\n",
      "📦 Fetching page 3 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 300\n",
      "📦 Fetching page 4 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 400\n",
      "📦 Fetching page 5 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 500\n",
      "📦 Fetching page 6 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 600\n",
      "📦 Fetching page 7 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 700\n",
      "📦 Fetching page 8 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 800\n",
      "📦 Fetching page 9 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 900\n",
      "📦 Fetching page 10 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1000\n",
      "📦 Fetching page 11 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1100\n",
      "📦 Fetching page 12 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1200\n",
      "📦 Fetching page 13 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1300\n",
      "📦 Fetching page 14 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1400\n",
      "📦 Fetching page 15 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1500\n",
      "📦 Fetching page 16 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1600\n",
      "📦 Fetching page 17 from 1483228800 to 1498867200...\n",
      "✅ Collected so far: 1700\n",
      "📦 Fetching page 1 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 1800\n",
      "📦 Fetching page 2 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 1900\n",
      "📦 Fetching page 3 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2000\n",
      "📦 Fetching page 4 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2100\n",
      "📦 Fetching page 5 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2200\n",
      "📦 Fetching page 6 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2300\n",
      "📦 Fetching page 7 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2400\n",
      "📦 Fetching page 8 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2500\n",
      "📦 Fetching page 9 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2600\n",
      "📦 Fetching page 10 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2700\n",
      "📦 Fetching page 11 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2800\n",
      "📦 Fetching page 12 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 2900\n",
      "📦 Fetching page 13 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 3000\n",
      "📦 Fetching page 14 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 3100\n",
      "📦 Fetching page 15 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 3200\n",
      "📦 Fetching page 16 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 3300\n",
      "📦 Fetching page 17 from 1498867201 to 1514764800...\n",
      "✅ Collected so far: 3400\n",
      "📦 Fetching page 1 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 3500\n",
      "📦 Fetching page 2 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 3600\n",
      "📦 Fetching page 3 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 3700\n",
      "📦 Fetching page 4 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 3800\n",
      "📦 Fetching page 5 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 3900\n",
      "📦 Fetching page 6 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4000\n",
      "📦 Fetching page 7 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4100\n",
      "📦 Fetching page 8 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4200\n",
      "📦 Fetching page 9 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4300\n",
      "📦 Fetching page 10 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4400\n",
      "📦 Fetching page 11 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4500\n",
      "📦 Fetching page 12 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4600\n",
      "📦 Fetching page 13 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4700\n",
      "📦 Fetching page 14 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4800\n",
      "📦 Fetching page 15 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 4900\n",
      "📦 Fetching page 16 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 5000\n",
      "📦 Fetching page 17 from 1514764801 to 1530403200...\n",
      "✅ Collected so far: 5100\n",
      "📦 Fetching page 1 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5200\n",
      "📦 Fetching page 2 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5300\n",
      "📦 Fetching page 3 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5400\n",
      "📦 Fetching page 4 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5500\n",
      "📦 Fetching page 5 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5600\n",
      "📦 Fetching page 6 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5700\n",
      "📦 Fetching page 7 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5800\n",
      "📦 Fetching page 8 from 1530403201 to 1546300800...\n",
      "✅ Collected so far: 5900\n",
      "📦 Fetching page 9 from 1530403201 to 1546300800...\n",
      "❌ API limit hit or error: {'error_id': 502, 'error_message': 'too many requests from this IP, more requests available in 82994 seconds', 'error_name': 'throttle_violation'}\n",
      "📦 Fetching page 1 from 1546300801 to 1561939200...\n",
      "❌ API limit hit or error: {'error_id': 502, 'error_message': 'too many requests from this IP, more requests available in 82994 seconds', 'error_name': 'throttle_violation'}\n",
      "📦 Fetching page 1 from 1561939201 to 1577836800...\n",
      "❌ API limit hit or error: {'error_id': 502, 'error_message': 'too many requests from this IP, more requests available in 82994 seconds', 'error_name': 'throttle_violation'}\n",
      "✅ Saved 10,000 rows to 'stackoverflow_2017_2019_10k.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html):\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "titles, bodies, tags_list = [], [], []\n",
    "\n",
    "# 2017–2019: 6 half-year date ranges\n",
    "date_ranges = [\n",
    "    (1483228800, 1498867200),  # Jan–Jun 2017\n",
    "    (1498867201, 1514764800),  # Jul–Dec 2017\n",
    "    (1514764801, 1530403200),  # Jan–Jun 2018\n",
    "    (1530403201, 1546300800),  # Jul–Dec 2018\n",
    "    (1546300801, 1561939200),  # Jan–Jun 2019\n",
    "    (1561939201, 1577836800),  # Jul–Dec 2019\n",
    "]\n",
    "\n",
    "api_key = \"\"  # Optional\n",
    "\n",
    "for start, end in date_ranges:\n",
    "    for page in range(1, 18):  # 17 pages × 100 × 6 ≈ 10,200\n",
    "        print(f\"📦 Fetching page {page} from {start} to {end}...\")\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": 100,\n",
    "            \"fromdate\": start,\n",
    "            \"todate\": end,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"filter\": \"withbody\"\n",
    "        }\n",
    "        if api_key:\n",
    "            params[\"key\"] = api_key\n",
    "\n",
    "        resp = requests.get(\"https://api.stackexchange.com/2.3/questions\", params=params)\n",
    "        data = resp.json()\n",
    "\n",
    "        if \"items\" not in data:\n",
    "            print(\"❌ API limit hit or error:\", data)\n",
    "            break\n",
    "\n",
    "        for item in data[\"items\"]:\n",
    "            titles.append(item.get(\"title\", \"\"))\n",
    "            bodies.append(clean_html(item.get(\"body\", \"\")))\n",
    "            tags_list.append(item.get(\"tags\", []))\n",
    "\n",
    "        print(f\"✅ Collected so far: {len(titles)}\")\n",
    "        time.sleep(1.2)\n",
    "\n",
    "# Save new data\n",
    "df_new = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Body\": bodies,\n",
    "    \"Tags\": tags_list\n",
    "})\n",
    "df_new.to_csv(\"stackoverflow_2017_2019_10k.csv\", index=False)\n",
    "print(\"✅ Saved 10,000 rows to 'stackoverflow_2017_2019_10k.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6049e6c0-74db-45c4-a91f-9b58f71e529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Fetching page 1 from 1356998400 to 1372636800...\n",
      "❌ API limit hit or no items: {'error_id': 502, 'error_message': 'too many requests from this IP, more requests available in 82651 seconds', 'error_name': 'throttle_violation'}\n",
      "🔄 Fetching page 1 from 1372636801 to 1388534400...\n",
      "❌ API limit hit or no items: {'error_id': 502, 'error_message': 'too many requests from this IP, more requests available in 82650 seconds', 'error_name': 'throttle_violation'}\n",
      "🔄 Fetching page 1 from 1388534401 to 1404172800...\n",
      "❌ API limit hit or no items: {'error_id': 502, 'error_message': 'too many requests from this IP, more requests available in 82650 seconds', 'error_name': 'throttle_violation'}\n",
      "🔄 Fetching page 1 from 1404172801 to 1420070400...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m     39\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.stackexchange.com/2.3/questions\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m---> 40\u001b[0m data \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ API limit hit or no items:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html):\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "titles, bodies, tags_list = [], [], []\n",
    "\n",
    "# Date ranges for 2013–2015 (Jan–Jun, Jul–Dec)\n",
    "date_ranges = [\n",
    "    (1356998400, 1372636800),  # Jan–Jun 2013\n",
    "    (1372636801, 1388534400),  # Jul–Dec 2013\n",
    "    (1388534401, 1404172800),  # Jan–Jun 2014\n",
    "    (1404172801, 1420070400),  # Jul–Dec 2014\n",
    "    (1420070401, 1435708800),  # Jan–Jun 2015\n",
    "    (1435708801, 1451606400),  # Jul–Dec 2015\n",
    "]\n",
    "\n",
    "api_key = \"\"  # Optional\n",
    "\n",
    "for start, end in date_ranges:\n",
    "    for page in range(1, 8):  # 7 pages × 100 = ~700 per range\n",
    "        print(f\"🔄 Fetching page {page} from {start} to {end}...\")\n",
    "        params = {\n",
    "            \"page\": page,\n",
    "            \"pagesize\": 100,\n",
    "            \"fromdate\": start,\n",
    "            \"todate\": end,\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"filter\": \"withbody\"\n",
    "        }\n",
    "        if api_key:\n",
    "            params[\"key\"] = api_key\n",
    "\n",
    "        resp = requests.get(\"https://api.stackexchange.com/2.3/questions\", params=params)\n",
    "        data = resp.json()\n",
    "\n",
    "        if \"items\" not in data:\n",
    "            print(\"❌ API limit hit or no items:\", data)\n",
    "            break\n",
    "\n",
    "        for item in data[\"items\"]:\n",
    "            titles.append(item.get(\"title\", \"\"))\n",
    "            bodies.append(clean_html(item.get(\"body\", \"\")))\n",
    "            tags_list.append(item.get(\"tags\", []))\n",
    "\n",
    "        print(f\"✅ Total collected so far: {len(titles)}\")\n",
    "        time.sleep(1.2)  # Respect API limits\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Body\": bodies,\n",
    "    \"Tags\": tags_list\n",
    "})\n",
    "df.to_csv(\"stackoverflow_2013_2014_2015_4k.csv\", index=False)\n",
    "print(\"🎉 Done! ~4,000 questions saved to 'stackoverflow_2013_2014_2015_4k.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbf12ea9-b594-4257-a372-ae59fc8f67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"stackoverflow_2017_2019_10k.csv\")\n",
    "df2 = pd.read_csv(\"stackoverflow_2020_2021_2023_2024.csv\")\n",
    "final = pd.concat([df1, df2]).drop_duplicates().reset_index(drop=True)\n",
    "final.to_csv(\"stackoverflow_30k.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9421895-5233-40de-87a9-608a93459ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_csv(\"stackoverflow_30k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee9d1f7f-d3a8-4fef-8702-1fac983a4974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SQL/JS How to re-arrange duplicate rows into one</td>\n",
       "      <td>How can I regroup all cells that belongs to Us...</td>\n",
       "      <td>['php', 'jquery', 'sql']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scraping Ajax based Review Page with Scrapy</td>\n",
       "      <td>There. I am trying to scrape a website. Everyt...</td>\n",
       "      <td>['python', 'ajax', 'scrapy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python - mock imported dictionary</td>\n",
       "      <td>At the top of the code I want to test I have a...</td>\n",
       "      <td>['python', 'unit-testing', 'dictionary', 'mock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angular ui.select module is not available?</td>\n",
       "      <td>Trying to setup the ui-select angular directiv...</td>\n",
       "      <td>['javascript', 'html', 'css', 'angularjs', 'ui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Java - error in prompts</td>\n",
       "      <td>This is what I'm trying to do: Write a program...</td>\n",
       "      <td>['java', 'output']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25895</th>\n",
       "      <td>Getting extra padding/margin around multi-line...</td>\n",
       "      <td>I have the following code\\n\\n\\n.menu {\\n    ma...</td>\n",
       "      <td>['html', 'css']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25896</th>\n",
       "      <td>Difference between two dates - method missing ...</td>\n",
       "      <td>The code is as follows\\npickup_time = DateTime...</td>\n",
       "      <td>['ruby-on-rails']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25897</th>\n",
       "      <td>How to bold text between 1. and :</td>\n",
       "      <td>One of the functions of the library is to auto...</td>\n",
       "      <td>['javascript', 'html', 'regex']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25898</th>\n",
       "      <td>rememberAsyncImagePainter is not loading image...</td>\n",
       "      <td>I am trying to load the images of different us...</td>\n",
       "      <td>['android', 'image', 'kotlin', 'android-jetpac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25899</th>\n",
       "      <td>Unable to get predefined source branch variabl...</td>\n",
       "      <td>$ echo \"Source branch:$CI_MERGE_REQUEST_BRANCH...</td>\n",
       "      <td>['gitlab', 'gitlab-ci', 'pipeline', 'cicd']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25900 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0       SQL/JS How to re-arrange duplicate rows into one   \n",
       "1            Scraping Ajax based Review Page with Scrapy   \n",
       "2                      Python - mock imported dictionary   \n",
       "3             angular ui.select module is not available?   \n",
       "4                                Java - error in prompts   \n",
       "...                                                  ...   \n",
       "25895  Getting extra padding/margin around multi-line...   \n",
       "25896  Difference between two dates - method missing ...   \n",
       "25897                  How to bold text between 1. and :   \n",
       "25898  rememberAsyncImagePainter is not loading image...   \n",
       "25899  Unable to get predefined source branch variabl...   \n",
       "\n",
       "                                                    Body  \\\n",
       "0      How can I regroup all cells that belongs to Us...   \n",
       "1      There. I am trying to scrape a website. Everyt...   \n",
       "2      At the top of the code I want to test I have a...   \n",
       "3      Trying to setup the ui-select angular directiv...   \n",
       "4      This is what I'm trying to do: Write a program...   \n",
       "...                                                  ...   \n",
       "25895  I have the following code\\n\\n\\n.menu {\\n    ma...   \n",
       "25896  The code is as follows\\npickup_time = DateTime...   \n",
       "25897  One of the functions of the library is to auto...   \n",
       "25898  I am trying to load the images of different us...   \n",
       "25899  $ echo \"Source branch:$CI_MERGE_REQUEST_BRANCH...   \n",
       "\n",
       "                                                    Tags  \n",
       "0                               ['php', 'jquery', 'sql']  \n",
       "1                           ['python', 'ajax', 'scrapy']  \n",
       "2      ['python', 'unit-testing', 'dictionary', 'mock...  \n",
       "3      ['javascript', 'html', 'css', 'angularjs', 'ui...  \n",
       "4                                     ['java', 'output']  \n",
       "...                                                  ...  \n",
       "25895                                    ['html', 'css']  \n",
       "25896                                  ['ruby-on-rails']  \n",
       "25897                    ['javascript', 'html', 'regex']  \n",
       "25898  ['android', 'image', 'kotlin', 'android-jetpac...  \n",
       "25899        ['gitlab', 'gitlab-ci', 'pipeline', 'cicd']  \n",
       "\n",
       "[25900 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cec40-7d4a-4792-a3e8-ebb0e6760e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
